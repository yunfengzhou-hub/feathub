{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alibaba/feathub/blob/master/docs/examples/fraud_detection.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "\n",
    "This notebook illustrates the use of FeatHub to create a model that predicts the\n",
    "fraud status of transactions based on the user account data and trasaction data.\n",
    "The main focus of this notebook is to depict:\n",
    "\n",
    "- How a feature designer can define heterogenious features from different data\n",
    "  sources (user account data and transaction data) with different keys by using\n",
    "  FeatHub, and\n",
    "- How a feature consumer can extract features using multiple FeatureView.\n",
    "\n",
    "The sample fraud transaction datasets that are used in the notebook can be found\n",
    "here: https://github.com/microsoft/r-server-fraud-detection.\n",
    "\n",
    "Please feel free to view this example interactively with Colab by clicking the\n",
    "badge at the top left corner of this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "This example has been verified in Python 3.7 with the following libraries.\n",
    "\n",
    "- feathub-nightly[spark]\n",
    "- plotly\n",
    "- matplotlib\n",
    "\n",
    "Execute the following cells to install these dependencies. **If the notebook is\n",
    "executed in Colab, restart the runtime after the following cells are executed,\n",
    "in order to make sure Python 3.7 is correctly configured to execute the Python\n",
    "cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python_version=`python -V`\n",
    "if [[ $python_version != *\"3.7\"* ]]; then\n",
    "    # install python 3.7\n",
    "    sudo apt-get update -y\n",
    "    sudo apt-get install python3.7 python3-pip python3.7-distutils python3-apt\n",
    "\n",
    "    # change alternatives\n",
    "    sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 0\n",
    "    sudo update-alternatives --set python3 /usr/bin/python3.7\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "feathub_dependencies=`pip list | grep feathub`\n",
    "if [[ -z \"$feathub_dependencies\" ]]; then\n",
    "    pip install \"feathub-nightly[flink]\"\n",
    "fi\n",
    "\n",
    "pip install plotly matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "from typing import OrderedDict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from feathub.common import types\n",
    "from feathub.feathub_client import FeathubClient\n",
    "from feathub.feature_tables.sources.file_system_source import FileSystemSource\n",
    "from feathub.feature_views.derived_feature_view import DerivedFeatureView\n",
    "from feathub.feature_views.feature import Feature\n",
    "from feathub.feature_views.transforms.over_window_transform import OverWindowTransform\n",
    "from feathub.table.schema import Schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initliaze FeatHub client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = FeathubClient(\n",
    "    props={\n",
    "        \"processor\": {\n",
    "            \"type\": \"spark\",\n",
    "            \"spark\": {\n",
    "                \"master\": \"local[1]\",\n",
    "            },\n",
    "        },\n",
    "        \"registry\": {\n",
    "            \"type\": \"local\",\n",
    "            \"local\": {\n",
    "                \"namespace\": \"default\",\n",
    "            },\n",
    "        },\n",
    "        \"feature_service\": {\n",
    "            \"type\": \"local\",\n",
    "            \"local\": {},\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess source data\n",
    "\n",
    "We prepare the fraud detection dataset as follows:\n",
    "\n",
    "1. Download Account info data, fraud transactions data, and untagged\n",
    "   transactions data.\n",
    "2. Tag transaction data based on the fraud transactions data.\n",
    "   1. Aggregate the Fraud table on the account level, creating a start and end\n",
    "      datetime.\n",
    "   2. Join this data with the untagged data.\n",
    "   3. Tag the data: is_fraud = 0 for non fraud, 1 for fraud.\n",
    "3. Save the result data files to local filesystem so that FeatHub jobs can\n",
    "   consume.\n",
    "\n",
    "To learn more about the fraud detection scenario as well as the dataset source\n",
    "we use and the method we tag the transactions, please see\n",
    "[here](https://microsoft.github.io/r-server-fraud-detection/data-scientist.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_file(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    file_name = os.path.basename(parsed_url.path)\n",
    "\n",
    "    if os.path.isfile(file_name):\n",
    "        return\n",
    "\n",
    "    r = requests.get(url)\n",
    "    open(file_name, \"wb\").write(r.content)\n",
    "\n",
    "\n",
    "file_path = \"https://raw.github.com/microsoft/r-server-fraud-detection/master/Data/\"\n",
    "maybe_download_file(file_path + \"Account_Info.csv\")\n",
    "maybe_download_file(file_path + \"Fraud_Transactions.csv\")\n",
    "maybe_download_file(file_path + \"Untagged_Transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fraud_df = pd.read_csv(\"Fraud_Transactions.csv\")\n",
    "obs_df = pd.read_csv(\"Untagged_Transactions.csv\")\n",
    "\n",
    "# Combine transactionDate and transactionTime into one column. E.g. \"20130903\", \"013641\" -> \"20130903 013641\"\n",
    "fraud_df[\"timestamp\"] = (\n",
    "    fraud_df[\"transactionDate\"].astype(str)\n",
    "    + \" \"\n",
    "    + fraud_df[\"transactionTime\"].astype(str).str.zfill(6)\n",
    ")\n",
    "obs_df[\"timestamp\"] = (\n",
    "    obs_df[\"transactionDate\"].astype(str)\n",
    "    + \" \"\n",
    "    + obs_df[\"transactionTime\"].astype(str).str.zfill(6)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we compute the timestamp range that the frauds were happened by\n",
    "referencing the transaction-level fraud data. We create the labels `is_fraud` to\n",
    "the untagged transaction data based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each user in the fraud transaction data, get the timestamp range that the fraud transactions were happened.\n",
    "fraud_labels_df = fraud_df.groupby(\"accountID\").agg({\"timestamp\": [\"min\", \"max\"]})\n",
    "fraud_labels_df.columns = [\"_\".join(col) for col in fraud_labels_df.columns]\n",
    "fraud_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud and untagged transaction data to generate the tagged transaction data.\n",
    "transactions_df = pd.concat([fraud_df, obs_df], ignore_index=True).merge(\n",
    "    fraud_labels_df,\n",
    "    on=\"accountID\",\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "# Data cleaning\n",
    "transactions_df.dropna(\n",
    "    subset=[\n",
    "        \"accountID\",\n",
    "        \"transactionID\",\n",
    "        \"transactionAmount\",\n",
    "        \"localHour\",\n",
    "        \"timestamp\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "transactions_df.sort_values(\"timestamp\", inplace=True)\n",
    "transactions_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# is_fraud = 0 if the transaction is not fraud. Otherwise (if it is a fraud), is_fraud = 1.\n",
    "transactions_df[\"is_fraud\"] = np.logical_and(\n",
    "    transactions_df[\"timestamp_min\"] <= transactions_df[\"timestamp\"],\n",
    "    transactions_df[\"timestamp\"] <= transactions_df[\"timestamp_max\"],\n",
    ").astype(int)\n",
    "\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df[\"is_fraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the data in transactions_df to accelerate calculation.\n",
    "sampled_df = transactions_df.sample(n=10000, random_state=1)\n",
    "\n",
    "transactions_df = pd.concat(\n",
    "    [\n",
    "        sampled_df[sampled_df[\"accountID\"] != \"A1055520452832600\"],\n",
    "        transactions_df[transactions_df[\"accountID\"] == \"A1055520452832600\"],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.to_csv(\"transactions.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df = pd.read_csv(\"Account_Info.csv\")\n",
    "account_df[\"timestamp\"] = (\n",
    "    account_df[\"transactionDate\"].astype(str)\n",
    "    + \" \"\n",
    "    + account_df[\"transactionTime\"].astype(str).str.zfill(6)\n",
    ")\n",
    "account_df.to_csv(\"accounts.csv\", index=False, header=False)\n",
    "account_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define following features:\n",
    "\n",
    "- Account features: Account-level features that will be joined to observation\n",
    "  data on accountID\n",
    "- Transaction features: The features that will be joined to observation data on\n",
    "  transactionID\n",
    "- Transaction aggregated features: The features aggregated by accountID\n",
    "- Derived features: The features derived from other features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define account features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_schema = (\n",
    "    Schema.new_builder()\n",
    "    .column(\"accountID\", types.String)\n",
    "    .column(\"transactionDate\", types.String)\n",
    "    .column(\"transactionTime\", types.String)\n",
    "    .column(\"accountOwnerName\", types.String)\n",
    "    .column(\"accountAddress\", types.String)\n",
    "    .column(\"accountPostalCode\", types.String)\n",
    "    .column(\"accountCity\", types.String)\n",
    "    .column(\"accountState\", types.String)\n",
    "    .column(\"accountCountry\", types.String)\n",
    "    .column(\"accountOpenDate\", types.String)\n",
    "    .column(\"accountAge\", types.Float64)\n",
    "    .column(\"isUserRegistered\", types.Bool)\n",
    "    .column(\"paymentInstrumentAgeInAccount\", types.String)\n",
    "    .column(\"numPaymentRejects1dPerUser\", types.Float64)\n",
    "    .column(\"timestamp\", types.String)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "account_source = FileSystemSource(\n",
    "    name=\"account_source\",\n",
    "    path=\"accounts.csv\",\n",
    "    data_format=\"csv\",\n",
    "    schema=account_schema,\n",
    "    keys=[\"accountID\"],\n",
    "    timestamp_field=\"timestamp\",\n",
    "    timestamp_format=\"%Y%m%d %H%M%S\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use `accountCountry`, `isUserRegistered`, `numPaymentRejects1dPerUser`,\n",
    "and `accountAge` as the account features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_feature_view_1 = DerivedFeatureView(\n",
    "    name=\"account_feature_view_1\",\n",
    "    source=account_source,\n",
    "    features=[\n",
    "        Feature(\n",
    "            name=feature_name,\n",
    "            transform=feature_name,\n",
    "            keys=[\"accountID\"],\n",
    "        )\n",
    "        for feature_name in [\n",
    "            \"accountID\",\n",
    "            \"accountCountry\",\n",
    "            \"isUserRegistered\",\n",
    "            \"numPaymentRejects1dPerUser\",\n",
    "            \"accountAge\",\n",
    "        ]\n",
    "    ],\n",
    "    keep_source_fields=True,\n",
    "    filter_expr=\"accountID IS NOT NULL\",\n",
    ")\n",
    "\n",
    "account_feature_view_2 = DerivedFeatureView(\n",
    "    name=\"account_feature_view_2\",\n",
    "    source=account_feature_view_1,\n",
    "    features=[\n",
    "        Feature(\n",
    "            name=\"account_country_code\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=\"accountCountry\",\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"is_user_registered\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=\"isUserRegistered\",\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"num_payment_rejects_1d_per_user\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=\"numPaymentRejects1dPerUser\",\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"account_age\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=\"accountAge\",\n",
    "        ),\n",
    "    ],\n",
    "    keep_source_fields=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transaction features\n",
    "\n",
    "We already checked the transaction dataset when we tagged the fraud label\n",
    "is_fraud. So, let's jump to defining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_schema = (\n",
    "    Schema.new_builder()\n",
    "    .column(\"transactionID\", types.String)\n",
    "    .column(\"accountID\", types.String)\n",
    "    .column(\"transactionAmount\", types.Float32)\n",
    "    .column(\"transactionCurrencyCode\", types.String)\n",
    "    .column(\"transactionDate\", types.String)\n",
    "    .column(\"transactionTime\", types.Float32)\n",
    "    .column(\"localHour\", types.Float32)\n",
    "    .column(\"transactionDeviceId\", types.String)\n",
    "    .column(\"transactionIPaddress\", types.String)\n",
    "    .column(\"timestamp\", types.String)\n",
    "    .column(\"transactionAmountUSD\", types.String)\n",
    "    .column(\"transactionCurrencyConversionRate\", types.String)\n",
    "    .column(\"transactionScenario\", types.String)\n",
    "    .column(\"transactionType\", types.String)\n",
    "    .column(\"transactionMethod\", types.String)\n",
    "    .column(\"transactionDeviceType\", types.String)\n",
    "    .column(\"ipState\", types.String)\n",
    "    .column(\"ipPostcode\", types.String)\n",
    "    .column(\"ipCountryCode\", types.String)\n",
    "    .column(\"isProxyIP\", types.Bool)\n",
    "    .column(\"browserType\", types.String)\n",
    "    .column(\"browserLanguage\", types.String)\n",
    "    .column(\"paymentInstrumentType\", types.String)\n",
    "    .column(\"cardType\", types.String)\n",
    "    .column(\"cardNumberInputMethod\", types.String)\n",
    "    .column(\"paymentInstrumentID\", types.String)\n",
    "    .column(\"paymentBillingAddress\", types.String)\n",
    "    .column(\"paymentBillingPostalCode\", types.String)\n",
    "    .column(\"paymentBillingState\", types.String)\n",
    "    .column(\"paymentBillingCountryCode\", types.String)\n",
    "    .column(\"paymentBillingName\", types.String)\n",
    "    .column(\"shippingAddress\", types.String)\n",
    "    .column(\"shippingPostalCode\", types.String)\n",
    "    .column(\"shippingCity\", types.String)\n",
    "    .column(\"shippingState\", types.String)\n",
    "    .column(\"shippingCountry\", types.String)\n",
    "    .column(\"cvvVerifyResult\", types.String)\n",
    "    .column(\"responseCode\", types.String)\n",
    "    .column(\"digitalItemCount\", types.String)\n",
    "    .column(\"physicalItemCount\", types.String)\n",
    "    .column(\"purchaseProductType\", types.String)\n",
    "    .column(\"timestamp_min\", types.String)\n",
    "    .column(\"timestamp_max\", types.String)\n",
    "    .column(\"is_fraud\", types.String)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "transaction_source = FileSystemSource(\n",
    "    name=\"transaction_source\",\n",
    "    path=\"transactions.csv\",\n",
    "    data_format=\"csv\",\n",
    "    schema=transaction_schema,\n",
    "    timestamp_field=\"timestamp\",\n",
    "    timestamp_format=\"%Y%m%d %H%M%S\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transaction aggregation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_feature_view = DerivedFeatureView(\n",
    "    name=\"transaction_feature_view\",\n",
    "    source=transaction_source,\n",
    "    features=[\n",
    "        Feature(\n",
    "            name=\"transaction_amount\",\n",
    "            keys=[\"transactionID\"],\n",
    "            transform=\"transactionAmount\",\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"transaction_country_code\",\n",
    "            keys=[\"transactionID\"],\n",
    "            transform=\"ipCountryCode\",\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"transaction_time\",\n",
    "            keys=[\"transactionID\"],\n",
    "            transform=\"localHour\",  # Local time of the transaction\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"is_proxy_ip\",\n",
    "            keys=[\"transactionID\"],\n",
    "            transform=\"isProxyIP\",  # [nan, True, False]\n",
    "        ),\n",
    "        Feature(\n",
    "            name=\"cvv_verify_result\",\n",
    "            keys=[\"transactionID\"],\n",
    "            transform=\"cvvVerifyResult\",  # [nan, 'M', 'P', 'N', 'X', 'U', 'S', 'Y']\n",
    "        ),\n",
    "    ],\n",
    "    keep_source_fields=True,\n",
    ")\n",
    "\n",
    "transaction_agg_feature_view = DerivedFeatureView(\n",
    "    name=\"transaction_agg_feature_view\",\n",
    "    source=transaction_source,\n",
    "    features=[\n",
    "        Feature(\n",
    "            name=\"avg_transaction_amount\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=OverWindowTransform(\n",
    "                expr=\"transactionAmount\",\n",
    "                agg_func=\"AVG\",\n",
    "                window_size=timedelta(days=7),\n",
    "                group_by_keys=[\"accountID\"],\n",
    "            ),\n",
    "        ),\n",
    "        # number of transaction that took place in a day\n",
    "        Feature(\n",
    "            name=\"num_transaction_count_in_day\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=OverWindowTransform(\n",
    "                expr=\"transactionID\",\n",
    "                agg_func=\"COUNT\",\n",
    "                window_size=timedelta(days=1),\n",
    "                group_by_keys=[\"accountID\"],\n",
    "            ),\n",
    "        ),\n",
    "        # number of transaction that took place in the past week\n",
    "        Feature(\n",
    "            name=\"num_transaction_count_in_week\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=OverWindowTransform(\n",
    "                expr=\"transactionID\",\n",
    "                agg_func=\"COUNT\",\n",
    "                window_size=timedelta(days=7),\n",
    "                group_by_keys=[\"accountID\"],\n",
    "            ),\n",
    "        ),\n",
    "        # amount of transaction that took place in a day\n",
    "        Feature(\n",
    "            name=\"total_transaction_amount_in_day\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=OverWindowTransform(\n",
    "                expr=\"transactionAmount\",\n",
    "                agg_func=\"SUM\",\n",
    "                window_size=timedelta(days=1),\n",
    "                group_by_keys=[\"accountID\"],\n",
    "            ),\n",
    "        ),\n",
    "        # average time of transaction in the past week\n",
    "        Feature(\n",
    "            name=\"avg_transaction_time_in_week\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=OverWindowTransform(\n",
    "                expr=\"localHour\",\n",
    "                agg_func=\"AVG\",\n",
    "                window_size=timedelta(days=7),\n",
    "                group_by_keys=[\"accountID\"],\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    keep_source_fields=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive features from account and transaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_feature_view = DerivedFeatureView(\n",
    "    name=\"online_feature_view\",\n",
    "    source=transaction_agg_feature_view,\n",
    "    features=[\n",
    "        \"account_feature_view_2.account_country_code\",\n",
    "        \"account_feature_view_2.is_user_registered\",\n",
    "        \"account_feature_view_2.num_payment_rejects_1d_per_user\",\n",
    "        \"account_feature_view_2.account_age\",\n",
    "    ],\n",
    "    keep_source_fields=True,\n",
    ")\n",
    "\n",
    "diff_feature_view = DerivedFeatureView(\n",
    "    name=\"diff_feature_view\",\n",
    "    source=online_feature_view,\n",
    "    features=[\n",
    "        \"transaction_feature_view.transaction_amount\",\n",
    "        Feature(\n",
    "            name=\"diff_between_current_and_avg_amount\",\n",
    "            keys=[\"accountID\"],\n",
    "            transform=\"transaction_amount - avg_transaction_amount\",\n",
    "        ),\n",
    "    ],\n",
    "    keep_source_fields=True,\n",
    ")\n",
    "\n",
    "derived_feature_view = DerivedFeatureView(\n",
    "    name=\"derived_feature_view\",\n",
    "    source=diff_feature_view,\n",
    "    features=[\n",
    "        \"account_country_code\",\n",
    "        \"is_user_registered\",\n",
    "        \"num_payment_rejects_1d_per_user\",\n",
    "        \"account_age\",\n",
    "        \"avg_transaction_amount\",\n",
    "        \"num_transaction_count_in_day\",\n",
    "        \"num_transaction_count_in_week\",\n",
    "        \"total_transaction_amount_in_day\",\n",
    "        \"avg_transaction_time_in_week\",\n",
    "        \"transaction_amount\",\n",
    "        \"transaction_feature_view.transaction_country_code\",\n",
    "        \"transaction_feature_view.transaction_time\",\n",
    "        \"transaction_feature_view.is_proxy_ip\",\n",
    "        \"transaction_feature_view.cvv_verify_result\",\n",
    "        \"diff_between_current_and_avg_amount\",\n",
    "        \"is_fraud\",\n",
    "        \"timestamp\",\n",
    "    ],\n",
    "    keep_source_fields=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and materialize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.build_features(\n",
    "    [\n",
    "        account_feature_view_2,\n",
    "        transaction_feature_view,\n",
    "        transaction_agg_feature_view,\n",
    "        online_feature_view,\n",
    "        derived_feature_view,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_df = client.get_features(derived_feature_view).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_df.sort_values(\"transactionID\").head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Fraud Detection Model\n",
    "\n",
    "We use [Random Forest\n",
    "Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "to build a fraud detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable_df = derived_df[\n",
    "    [\n",
    "        \"account_country_code\",\n",
    "        \"is_user_registered\",\n",
    "        \"num_payment_rejects_1d_per_user\",\n",
    "        \"account_age\",\n",
    "        \"avg_transaction_amount\",\n",
    "        \"num_transaction_count_in_day\",\n",
    "        \"num_transaction_count_in_week\",\n",
    "        \"total_transaction_amount_in_day\",\n",
    "        \"avg_transaction_time_in_week\",\n",
    "        \"transaction_amount\",\n",
    "        \"transaction_country_code\",\n",
    "        \"transaction_time\",\n",
    "        \"is_proxy_ip\",\n",
    "        \"cvv_verify_result\",\n",
    "        \"diff_between_current_and_avg_amount\",\n",
    "        \"is_fraud\",\n",
    "        \"timestamp\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "plottable_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unserstand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only sub-samples for simplicity\n",
    "NUM_SAMPLES_TO_PLOT = 5000\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    plottable_df.sample(n=NUM_SAMPLES_TO_PLOT, random_state=42),\n",
    "    dimensions=plottable_df.columns[:-2],  # exclude the label and timestamp\n",
    "    color=\"is_fraud\",\n",
    "    labels={\n",
    "        col: col.replace(\"_\", \" \") for col in plottable_df.columns\n",
    "    },  # remove underscore\n",
    ")\n",
    "fig.update_traces(\n",
    "    diagonal_visible=False, showupperhalf=False, marker_size=3, marker_opacity=0.5\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=2000,\n",
    "    height=2000,\n",
    "    title={\"text\": \"Scatter matrix for transaction dataset\", \"font_size\": 20},\n",
    "    font_size=6,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(plottable_df) * 0.7)\n",
    "\n",
    "train_df = plottable_df.iloc[:n_train]\n",
    "test_df = plottable_df.iloc[n_train:]\n",
    "\n",
    "print(\n",
    "    f\"\"\"Training set:\n",
    "{train_df[\"is_fraud\"].value_counts()}\n",
    "\n",
    "Validation set:\n",
    "{test_df[\"is_fraud\"].value_counts()}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the time range of the training and test set doesn't overlap\n",
    "train_df[\"timestamp\"].max(), test_df[\"timestamp\"].min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels as integers\n",
    "y_train = train_df[\"is_fraud\"].astype(int).to_numpy()\n",
    "y_test = test_df[\"is_fraud\"].astype(int).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert categorical features into integer values by using one-hot-encoding and ordinal-encoding\n",
    "categorical_feature_names = [\n",
    "    \"account_country_code\",\n",
    "    \"transaction_country_code\",\n",
    "    \"cvv_verify_result\",\n",
    "]\n",
    "ordinal_feature_names = [\n",
    "    \"is_user_registered\",\n",
    "    \"is_proxy_ip\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder: OneHotEncoder = OneHotEncoder(sparse=False).fit(\n",
    "    plottable_df[categorical_feature_names]\n",
    ")\n",
    "ordinal_encoder: OrdinalEncoder = OrdinalEncoder().fit(\n",
    "    plottable_df[ordinal_feature_names]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(\n",
    "    (\n",
    "        one_hot_encoder.transform(train_df[categorical_feature_names]),\n",
    "        ordinal_encoder.transform(train_df[ordinal_feature_names]),\n",
    "        train_df.drop(\n",
    "            categorical_feature_names\n",
    "            + ordinal_feature_names\n",
    "            + [\"is_fraud\", \"timestamp\"],\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .to_numpy(),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "X_test = np.concatenate(\n",
    "    (\n",
    "        one_hot_encoder.transform(test_df[categorical_feature_names]),\n",
    "        ordinal_encoder.transform(test_df[ordinal_feature_names]),\n",
    "        test_df.drop(\n",
    "            categorical_feature_names\n",
    "            + ordinal_feature_names\n",
    "            + [\"is_fraud\", \"timestamp\"],\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .to_numpy(),\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = clf.predict_proba(X_test)\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_predictions(\n",
    "    y_test, y_prob[:, 1], name=\"RandomForestClassifier\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"Fraud Detection Precision-Recall Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\n",
    "    f\"\"\"Precision: {precision},\n",
    "Recall: {recall},\n",
    "F1: {f1}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_names = [\n",
    "    name\n",
    "    for name in train_df.columns\n",
    "    if name\n",
    "    not in set(\n",
    "        categorical_feature_names + ordinal_feature_names + [\"is_fraud\", \"timestamp\"]\n",
    "    )\n",
    "]\n",
    "numeric_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the order of features is [categorical features, ordinal features, numeric features]\n",
    "importances = clf.feature_importances_[-len(numeric_feature_names) :]\n",
    "std = np.std(\n",
    "    [\n",
    "        tree.feature_importances_[-len(numeric_feature_names) :]\n",
    "        for tree in clf.estimators_\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    pd.DataFrame(\n",
    "        [numeric_feature_names, importances, std],\n",
    "        index=[\"Numeric features\", \"importances\", \"std\"],\n",
    "    ).T,\n",
    "    y=\"Numeric features\",\n",
    "    x=\"importances\",\n",
    "    error_x=\"std\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Importance of the numeric features\",\n",
    ")\n",
    "fig.update_layout(showlegend=False, width=1000)\n",
    "fig.update_xaxes(title_text=\"Mean decrease in impurity\", range=[0, 0.5])\n",
    "fig.update_yaxes(title_text=\"Numeric features\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = categorical_feature_names + ordinal_feature_names\n",
    "categories = one_hot_encoder.categories_ + ordinal_encoder.categories_\n",
    "\n",
    "start_i = 0\n",
    "n_rows = len(feature_names)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=n_rows,\n",
    "    cols=1,\n",
    "    subplot_titles=[name.replace(\"_\", \" \") for name in feature_names],\n",
    "    x_title=\"Mean decrease in impurity\",\n",
    ")\n",
    "\n",
    "for i in range(n_rows):\n",
    "    category = categories[i]\n",
    "    end_i = start_i + len(category)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=clf.feature_importances_[start_i:end_i],\n",
    "            y=category,\n",
    "            width=0.2,\n",
    "            error_x=dict(\n",
    "                type=\"data\",\n",
    "                array=np.std(\n",
    "                    [\n",
    "                        tree.feature_importances_[start_i:end_i]\n",
    "                        for tree in clf.estimators_\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                ),\n",
    "            ),\n",
    "            orientation=\"h\",\n",
    "        ),\n",
    "        row=i + 1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    start_i = end_i\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Importance of the categorical features\",\n",
    "    showlegend=False,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.update_xaxes(range=[0, 0.5])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialize features in memory online store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feathub.feature_tables.sinks.memory_store_sink import MemoryStoreSink\n",
    "from feathub.feature_tables.sources.memory_store_source import MemoryStoreSource\n",
    "from feathub.feature_views.on_demand_feature_view import OnDemandFeatureView\n",
    "\n",
    "sink = MemoryStoreSink(table_name=\"table_name_1\")\n",
    "\n",
    "job = client.materialize_features(\n",
    "    feature_descriptor=online_feature_view,\n",
    "    sink=sink,\n",
    "    allow_overwrite=True,\n",
    ")\n",
    "job.wait(timeout_ms=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = MemoryStoreSource(\n",
    "    name=\"online_store_source\",\n",
    "    keys=[\"accountID\"],\n",
    "    table_name=\"table_name_1\",\n",
    ")\n",
    "on_demand_feature_view = OnDemandFeatureView(\n",
    "    name=\"on_demand_feature_view\",\n",
    "    features=[\n",
    "        \"online_store_source.\" + x\n",
    "        for x in [\n",
    "            \"is_user_registered\",\n",
    "            \"num_payment_rejects_1d_per_user\",\n",
    "            \"account_age\",\n",
    "            \"avg_transaction_amount\",\n",
    "            \"num_transaction_count_in_day\",\n",
    "            \"num_transaction_count_in_week\",\n",
    "            \"total_transaction_amount_in_day\",\n",
    "            \"avg_transaction_time_in_week\",\n",
    "        ]\n",
    "    ],\n",
    "    request_schema=Schema.new_builder().column(\"accountID\", types.String).build(),\n",
    ")\n",
    "client.build_features([source, on_demand_feature_view])\n",
    "\n",
    "request_df = pd.DataFrame(\n",
    "    np.array([[\"A1055520452832600\"]]),\n",
    "    columns=[\"accountID\"],\n",
    ")\n",
    "online_features = client.get_online_features(\n",
    "    request_df=request_df,\n",
    "    feature_view=on_demand_feature_view,\n",
    ")\n",
    "\n",
    "online_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feathub-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
